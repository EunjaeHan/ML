{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZFN5N5PGmr2",
        "outputId": "10c141f8-03e7-425f-fc54-1eeeca4e2b5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 공통: 데이터 전처리 & 기본 세팅 (Keras 기준)"
      ],
      "metadata": {
        "id": "R00bf34CI3qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# 1) 데이터 불러오기\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/WISDM.csv\", header=None)\n",
        "df.columns = [\"idx\", \"user\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"]\n",
        "\n",
        "# 2) 필요없는 열 정리 (idx는 버려도 됨)\n",
        "df = df[[\"user\", \"activity\", \"timestamp\", \"x\", \"y\", \"z\"]]\n",
        "\n",
        "# 3) 결측치 제거 (있다면)\n",
        "df = df.dropna()\n",
        "\n",
        "# 4) activity를 숫자 레이블로 변환\n",
        "activities, y_all = np.unique(df[\"activity\"], return_inverse=True)\n",
        "activity_to_idx = {a: i for i, a in enumerate(activities)}\n",
        "print(\"활동 레이블 매핑:\", activity_to_idx)\n",
        "\n",
        "df[\"activity_id\"] = y_all\n",
        "\n",
        "# 5) user, timestamp 기준으로 정렬 (안전하게)\n",
        "df = df.sort_values([\"user\", \"timestamp\"]).reset_index(drop=True)\n",
        "\n",
        "# 6) 시퀀스 생성 함수 (슬라이딩 윈도우)\n",
        "def create_sequences(df, window_size=128, step=64):\n",
        "    X_list = []\n",
        "    y_list = []\n",
        "\n",
        "    for user_id, user_df in df.groupby(\"user\"):\n",
        "        user_df = user_df.reset_index(drop=True)\n",
        "        signals = user_df[[\"x\", \"y\", \"z\"]].values\n",
        "        labels = user_df[\"activity_id\"].values\n",
        "\n",
        "        for start in range(0, len(user_df) - window_size + 1, step):\n",
        "            end = start + window_size\n",
        "            window_x = signals[start:end]            # (window_size, 3)\n",
        "            window_y = labels[start:end]\n",
        "            # 윈도우 안에서 가장 많이 나온 활동을 레이블로 사용\n",
        "            label = np.bincount(window_y).argmax()\n",
        "\n",
        "            X_list.append(window_x)\n",
        "            y_list.append(label)\n",
        "\n",
        "    X = np.stack(X_list)    # (N, window_size, 3)\n",
        "    y = np.array(y_list)    # (N,)\n",
        "    return X, y\n",
        "\n",
        "X, y = create_sequences(df, window_size=128, step=64)\n",
        "print(\"X shape:\", X.shape, \"y shape:\", y.shape)\n",
        "\n",
        "# 7) train/val/test 분할\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "X_val, X_test, y_val, y_test = train_test_split(\n",
        "    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
        ")\n",
        "\n",
        "print(\"train:\", X_train.shape, \"val:\", X_val.shape, \"test:\", X_test.shape)\n",
        "\n",
        "# 8) PyTorch Dataset 정의\n",
        "class WisdmDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.from_numpy(X).float()      # (N, seq_len, 3)\n",
        "        self.y = torch.from_numpy(y).long()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 모델에 넣기 편하게 (batch, channels, seq_len)로 바꾸기 위해 여기선 (seq_len, 3) 그대로 넘기고,\n",
        "        # 모델 안에서 permute 해도 되고, 여기서 미리 permute 해도 됨.\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = WisdmDataset(X_train, y_train)\n",
        "val_dataset   = WisdmDataset(X_val, y_val)\n",
        "test_dataset  = WisdmDataset(X_test, y_test)\n",
        "\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n",
        "\n",
        "num_classes = len(activities)\n",
        "seq_len = X.shape[1]\n",
        "input_dim = 3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haNfSNtbjN5f",
        "outputId": "c16e104c-70e2-4530-b3c4-8073c8183632"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "활동 레이블 매핑: {'Downstairs': 0, 'Jogging': 1, 'Sitting': 2, 'Standing': 3, 'Upstairs': 4, 'Walking': 5}\n",
            "X shape: (16331, 128, 3) y shape: (16331,)\n",
            "train: (11431, 128, 3) val: (2450, 128, 3) test: (2450, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 공통 학습 루프 (하나 만들어두고 모델만 바꿔 끼우기)"
      ],
      "metadata": {
        "id": "J0mGvHnjjkOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10):\n",
        "    model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch = X_batch.to(device)   # (B, seq_len, 3)\n",
        "            y_batch = y_batch.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X_batch)       # (B, num_classes)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * X_batch.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc  = correct / total\n",
        "\n",
        "        # ---- validation ----\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X_batch, y_batch in val_loader:\n",
        "                X_batch = X_batch.to(device)\n",
        "                y_batch = y_batch.to(device)\n",
        "\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "\n",
        "                val_loss += loss.item() * X_batch.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                val_total += y_batch.size(0)\n",
        "                val_correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "        val_loss /= val_total\n",
        "        val_acc  = val_correct / val_total\n",
        "\n",
        "        print(f\"[Epoch {epoch+1}] \"\n",
        "              f\"train_loss={train_loss:.4f}, train_acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "Y7IuS3s0jN3P"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. 모델 1 – 1D CNN"
      ],
      "metadata": {
        "id": "xVRrpiqPjrIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN1D(nn.Module):\n",
        "    def __init__(self, input_dim=3, num_classes=6, seq_len=128):\n",
        "        super().__init__()\n",
        "        # 입력: (B, seq_len, 3) → (B, 3, seq_len)로 바꿔서 Conv1d에 넣을 것\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=32, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(32)\n",
        "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5, padding=2)\n",
        "        self.bn2   = nn.BatchNorm1d(64)\n",
        "        self.pool  = nn.MaxPool1d(kernel_size=2)\n",
        "        self.relu  = nn.ReLU()\n",
        "\n",
        "        # seq_len이 절반으로 줄어듦 (pool 1번)\n",
        "        self.fc = nn.Linear(64 * (seq_len // 2), num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, seq_len, 3) → (B, 3, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
        "        x = x.flatten(start_dim=1)\n",
        "        out = self.fc(x)\n",
        "        return out\n",
        "\n",
        "cnn_model = CNN1D(input_dim=input_dim, num_classes=num_classes, seq_len=seq_len)\n",
        "# cnn_model = train_model(cnn_model, train_loader, val_loader, num_epochs=10)"
      ],
      "metadata": {
        "id": "kkf96PQNjN1I"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model = train_model(cnn_model, train_loader, val_loader, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGIxCDxZlMvJ",
        "outputId": "59a1328a-7576-4a87-9d25-ce4801b9eb6b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.5407, train_acc=0.8030 | val_loss=0.2956, val_acc=0.9012\n",
            "[Epoch 2] train_loss=0.2570, train_acc=0.9105 | val_loss=0.3032, val_acc=0.8935\n",
            "[Epoch 3] train_loss=0.1871, train_acc=0.9355 | val_loss=0.2212, val_acc=0.9151\n",
            "[Epoch 4] train_loss=0.1309, train_acc=0.9553 | val_loss=0.1417, val_acc=0.9518\n",
            "[Epoch 5] train_loss=0.1065, train_acc=0.9656 | val_loss=0.2237, val_acc=0.9110\n",
            "[Epoch 6] train_loss=0.0812, train_acc=0.9731 | val_loss=0.1370, val_acc=0.9535\n",
            "[Epoch 7] train_loss=0.0637, train_acc=0.9815 | val_loss=0.1304, val_acc=0.9531\n",
            "[Epoch 8] train_loss=0.0506, train_acc=0.9856 | val_loss=0.1447, val_acc=0.9514\n",
            "[Epoch 9] train_loss=0.0390, train_acc=0.9898 | val_loss=0.1444, val_acc=0.9494\n",
            "[Epoch 10] train_loss=0.0365, train_acc=0.9904 | val_loss=0.2155, val_acc=0.9298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. 모델 2 – LSTM"
      ],
      "metadata": {
        "id": "86PjqvKCjvAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=64, num_layers=1, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, seq_len, 3)\n",
        "        out, (hn, cn) = self.lstm(x)   # out: (B, seq_len, 2*hidden)\n",
        "        # 마지막 타임스텝 상태 사용\n",
        "        last_hidden = out[:, -1, :]   # (B, 2*hidden)\n",
        "        logits = self.fc(last_hidden)\n",
        "        return logits\n",
        "\n",
        "lstm_model = LSTMModel(input_dim=input_dim, hidden_dim=64, num_layers=1, num_classes=num_classes)\n",
        "# lstm_model = train_model(lstm_model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "U0LJVYYyjNsX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_model = train_model(lstm_model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31NgnkO_lPGF",
        "outputId": "1795c474-cf8b-4c7d-db78-9d6d508ef64d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=1.1051, train_acc=0.6099 | val_loss=0.9452, val_acc=0.6996\n",
            "[Epoch 2] train_loss=0.7784, train_acc=0.7289 | val_loss=0.6639, val_acc=0.7588\n",
            "[Epoch 3] train_loss=0.6292, train_acc=0.7781 | val_loss=0.5616, val_acc=0.7951\n",
            "[Epoch 4] train_loss=0.5656, train_acc=0.7932 | val_loss=0.5118, val_acc=0.8102\n",
            "[Epoch 5] train_loss=0.5301, train_acc=0.8074 | val_loss=0.6509, val_acc=0.7518\n",
            "[Epoch 6] train_loss=0.4686, train_acc=0.8262 | val_loss=0.4127, val_acc=0.8420\n",
            "[Epoch 7] train_loss=0.4096, train_acc=0.8445 | val_loss=0.4062, val_acc=0.8388\n",
            "[Epoch 8] train_loss=0.3732, train_acc=0.8549 | val_loss=0.3520, val_acc=0.8661\n",
            "[Epoch 9] train_loss=0.3428, train_acc=0.8658 | val_loss=0.3314, val_acc=0.8665\n",
            "[Epoch 10] train_loss=0.3106, train_acc=0.8774 | val_loss=0.3065, val_acc=0.8812\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. 모델 3 – GRU"
      ],
      "metadata": {
        "id": "LWsGH59bj02P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim=3, hidden_dim=64, num_layers=1, num_classes=6):\n",
        "        super().__init__()\n",
        "        self.gru = nn.GRU(\n",
        "            input_size=input_dim,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, hn = self.gru(x)          # out: (B, seq_len, 2*hidden)\n",
        "        last_hidden = out[:, -1, :]    # (B, 2*hidden)\n",
        "        logits = self.fc(last_hidden)\n",
        "        return logits\n",
        "\n",
        "gru_model = GRUModel(input_dim=input_dim, hidden_dim=64, num_layers=1, num_classes=num_classes)\n",
        "# gru_model = train_model(gru_model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "9CDffocBj1-R"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gru_model = train_model(gru_model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnJ9ldHolROk",
        "outputId": "4c2a846d-a648-4b34-ac82-4fd0e9252dbf"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.9648, train_acc=0.6694 | val_loss=0.6469, val_acc=0.7747\n",
            "[Epoch 2] train_loss=0.6098, train_acc=0.7859 | val_loss=0.5542, val_acc=0.8069\n",
            "[Epoch 3] train_loss=0.5291, train_acc=0.8035 | val_loss=0.4671, val_acc=0.8216\n",
            "[Epoch 4] train_loss=0.4527, train_acc=0.8207 | val_loss=0.4025, val_acc=0.8371\n",
            "[Epoch 5] train_loss=0.3837, train_acc=0.8464 | val_loss=0.3487, val_acc=0.8551\n",
            "[Epoch 6] train_loss=0.3544, train_acc=0.8590 | val_loss=0.3795, val_acc=0.8469\n",
            "[Epoch 7] train_loss=0.3053, train_acc=0.8753 | val_loss=0.3061, val_acc=0.8792\n",
            "[Epoch 8] train_loss=0.2830, train_acc=0.8850 | val_loss=0.2785, val_acc=0.8808\n",
            "[Epoch 9] train_loss=0.2609, train_acc=0.8983 | val_loss=0.2667, val_acc=0.8947\n",
            "[Epoch 10] train_loss=0.2394, train_acc=0.9065 | val_loss=0.2673, val_acc=0.9004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. 모델 4 – CNN + LSTM (하이브리드)"
      ],
      "metadata": {
        "id": "I2qNDZKQj4Q1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_dim=3, num_classes=6, seq_len=128, conv_channels=32, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        # CNN part\n",
        "        self.conv1 = nn.Conv1d(input_dim, conv_channels, kernel_size=5, padding=2)\n",
        "        self.bn1   = nn.BatchNorm1d(conv_channels)\n",
        "        self.pool  = nn.MaxPool1d(2)\n",
        "        self.relu  = nn.ReLU()\n",
        "\n",
        "        # LSTM part (CNN 출력 채널 수를 입력 차원으로 사용)\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=conv_channels,\n",
        "            hidden_size=hidden_dim,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, seq_len, 3) → (B, 3, seq_len)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.relu(self.bn1(self.conv1(x)))  # (B, C, T)\n",
        "        x = self.pool(x)                        # (B, C, T/2)\n",
        "\n",
        "        # (B, C, T') → (B, T', C) for LSTM\n",
        "        x = x.permute(0, 2, 1)\n",
        "        out, (hn, cn) = self.lstm(x)\n",
        "        last_hidden = out[:, -1, :]\n",
        "        logits = self.fc(last_hidden)\n",
        "        return logits\n",
        "\n",
        "cnn_lstm_model = CNN_LSTM(input_dim=input_dim, num_classes=num_classes, seq_len=seq_len)\n",
        "# cnn_lstm_model = train_model(cnn_lstm_model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "0NuRl-dkj5Gh"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_lstm_model = train_model(cnn_lstm_model, train_loader, val_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chNrRRV7lS1Q",
        "outputId": "8a5ec60f-b5c2-4fad-d924-dc5cb2fc30c3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1] train_loss=0.9491, train_acc=0.6731 | val_loss=0.6370, val_acc=0.7939\n",
            "[Epoch 2] train_loss=0.6025, train_acc=0.7927 | val_loss=0.5060, val_acc=0.8200\n",
            "[Epoch 3] train_loss=0.5279, train_acc=0.8052 | val_loss=0.7284, val_acc=0.7257\n",
            "[Epoch 4] train_loss=0.4815, train_acc=0.8232 | val_loss=0.3918, val_acc=0.8584\n",
            "[Epoch 5] train_loss=0.3773, train_acc=0.8592 | val_loss=0.3083, val_acc=0.8808\n",
            "[Epoch 6] train_loss=0.3502, train_acc=0.8717 | val_loss=0.3043, val_acc=0.8906\n",
            "[Epoch 7] train_loss=0.3018, train_acc=0.8878 | val_loss=0.2535, val_acc=0.8971\n",
            "[Epoch 8] train_loss=0.2754, train_acc=0.8951 | val_loss=0.2851, val_acc=0.8922\n",
            "[Epoch 9] train_loss=0.2567, train_acc=0.9039 | val_loss=0.2225, val_acc=0.9167\n",
            "[Epoch 10] train_loss=0.2318, train_acc=0.9137 | val_loss=0.2073, val_acc=0.9233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. 모델 5 – Transformer (Encoder)"
      ],
      "metadata": {
        "id": "MP278o8Nj99L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=500):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)  # (max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)   # (1, max_len, d_model)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, seq_len, d_model)\n",
        "        seq_len = x.size(1)\n",
        "        x = x + self.pe[:, :seq_len, :]\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, input_dim=3, num_classes=6, seq_len=128, d_model=64, nhead=8, num_layers=2, dim_feedforward=128):\n",
        "        super().__init__()\n",
        "        # 입력 (3차원)을 d_model 차원으로 projection\n",
        "        self.input_fc = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_len=seq_len)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (B, seq_len, 3)\n",
        "        x = self.input_fc(x)          # (B, seq_len, d_model)\n",
        "        x = self.pos_encoder(x)       # (B, seq_len, d_model)\n",
        "        x = self.transformer_encoder(x)  # (B, seq_len, d_model)\n",
        "\n",
        "        # [CLS] 토큰이 없으니 간단히 평균 pooling 사용\n",
        "        x = x.mean(dim=1)             # (B, d_model)\n",
        "        logits = self.fc_out(x)       # (B, num_classes)\n",
        "        return logits\n",
        "\n",
        "transformer_model = TransformerModel(input_dim=input_dim, num_classes=num_classes, seq_len=seq_len)\n",
        "# transformer_model = train_model(transformer_model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "NAlUL9g7kAJu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = train_model(transformer_model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "uJ5rBH9hulfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###8. 테스트 세트 평가"
      ],
      "metadata": {
        "id": "OsTCQCR5kCyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in test_loader:\n",
        "            X_batch = X_batch.to(device)\n",
        "            y_batch = y_batch.to(device)\n",
        "            outputs = model(X_batch)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += y_batch.size(0)\n",
        "            correct += predicted.eq(y_batch).sum().item()\n",
        "\n",
        "    print(\"Test Accuracy:\", correct / total)\n",
        "# evaluate_model(cnn_model, test_loader)"
      ],
      "metadata": {
        "id": "l1skc6fUkEmH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(cnn_model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxMZA_VGlCex",
        "outputId": "c8434688-9b29-4961-8187-cec8420b39ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.3183673469387755\n"
          ]
        }
      ]
    }
  ]
}